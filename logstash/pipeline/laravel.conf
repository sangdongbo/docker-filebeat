# =============================================================================
# Logstash Pipeline 配置 - Laravel 日志处理管道
# =============================================================================
# 功能: 接收、解析、转换、路由 Laravel 应用日志
# 版本: Logstash 9.2.4
#
# Pipeline 工作流程:
# 1. Input  - 接收来自 Filebeat 的原始日志数据
# 2. Filter - 解析日志格式，提取字段，添加标签，数据富化
# 3. Output - 发送到 Elasticsearch 并按规则创建索引
#
# 日志格式支持:
# - Laravel 默认格式: [2026-01-20 10:30:45] local.INFO: 消息内容 {"context":"data"}
# - 自定义格式: 通过修改 grok pattern 适配
#
# 修改建议:
# - 如果日志格式不同，修改 grok pattern
# - 如果需要不同的索引策略，修改 output.elasticsearch.index
# - 如果 Elasticsearch 认证信息不同，修改 user 和 password
# =============================================================================

# =============================================================================
# INPUT - 数据输入源配置
# =============================================================================
# 作用: 定义 Logstash 从哪里接收数据
input {
  # ---------------------------------------------------------------------------
  # Beats 输入 (主要输入源)
  # ---------------------------------------------------------------------------
  # 作用: 接收来自 Filebeat、Metricbeat、Packetbeat 等 Beats 系列的数据
  # 协议: Lumberjack 协议（Beats 专用的高效二进制协议）
  beats {
    # 监听端口 (默认 5044)
    # 注意: 必须与 Filebeat 配置的 output.logstash.hosts 端口一致
    # 防火墙: 确保此端口在防火墙中开放
    port => 5044
    
    # SSL/TLS 配置
    # ssl_enabled => false: 禁用 SSL（开发环境）
    # ssl_enabled => true:  启用 SSL（生产环境推荐）
    ssl_enabled => false
    
    # 如果启用 SSL，需要配置证书（取消下面的注释）:
    # ssl_certificate => "/etc/logstash/certs/logstash.crt"  # 服务器证书
    # ssl_key => "/etc/logstash/certs/logstash.key"          # 私钥
    # ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]  # CA 证书
    # ssl_verify_mode => "force_peer"  # 强制验证客户端证书
    
    # 其他可选配置:
    # host => "0.0.0.0"              # 监听地址，0.0.0.0 表示所有网卡
    # ssl_handshake_timeout => 10000 # SSL 握手超时（毫秒）
    # congestion_threshold => 5      # 拥塞阈值
    # target_field_for_codec => "message"  # 目标字段
  }
  
  # ---------------------------------------------------------------------------
  # TCP 输入 (可选，用于测试和调试)
  # ---------------------------------------------------------------------------
  # 作用: 接收 JSON 格式的 TCP 数据
  # 使用场景:
  # - 测试日志处理流程
  # - 应用直接通过 TCP 发送日志
  # - 使用 telnet/nc 手动发送测试数据
  # 测试命令: echo '{"message":"test"}' | nc localhost 50000
  tcp {
    # 监听端口
    port => 50000
    
    # 编解码器: json 表示自动解析 JSON 格式的数据
    codec => json
    
    # 其他可选配置:
    # host => "0.0.0.0"     # 监听地址
    # ssl_enable => true    # 启用 SSL
    # mode => "server"      # server 或 client 模式
  }
  
  # ---------------------------------------------------------------------------
  # 其他输入源示例 (根据需要取消注释)
  # ---------------------------------------------------------------------------
  
  # HTTP 输入 - 接收 HTTP POST 请求
  # http {
  #   host => "0.0.0.0"
  #   port => 8080
  #   codec => json
  # }
  
  # File 输入 - 直接读取文件（类似 Filebeat）
  # file {
  #   path => "/var/log/laravel/*.log"
  #   start_position => "beginning"
  #   sincedb_path => "/dev/null"  # 每次都从头读取
  # }
  
  # Kafka 输入 - 从 Kafka 消费数据
  # kafka {
  #   bootstrap_servers => "kafka1:9092,kafka2:9092"
  #   topics => ["laravel-logs"]
  #   group_id => "logstash-consumer"
  #   codec => json
  # }
}

# =============================================================================
# FILTER - 日志解析和数据处理
# =============================================================================
# 作用: 对原始日志进行解析、转换、富化，提取结构化字段
# 处理顺序: 按照配置从上到下依次执行
# 条件判断: 可以使用 if/else 进行条件处理

filter {
  # ---------------------------------------------------------------------------
  # Laravel 日志解析 (核心处理逻辑)
  # ---------------------------------------------------------------------------
  # 条件: 只处理标记为 app=laravel 的日志（由 Filebeat 添加）
  # 作用: 避免处理非 Laravel 日志，提高效率
  if [app] == "laravel" {
    
    # -------------------------------------------------------------------------
    # Grok 解析器 - 提取日志字段
    # -------------------------------------------------------------------------
    # 作用: 使用正则表达式从非结构化的日志文本中提取结构化字段
    # Laravel 日志格式: [2026-01-20 10:30:45] local.INFO: 消息内容 {"context":"data"}
    # Grok Pattern 说明:
    #   - %{TIMESTAMP_ISO8601:timestamp}: 匹配 ISO8601 格式的时间戳，存入 timestamp 字段
    #   - %{DATA:environment}: 匹配任意字符（非贪婪），存入 environment 字段（如: local, production）
    #   - %{LOGLEVEL:level}: 匹配日志级别，存入 level 字段（如: INFO, ERROR, WARNING）
    #   - %{GREEDYDATA:log_message}: 匹配剩余所有内容（贪婪），存入 log_message 字段
    grok {
      # match: 定义匹配规则
      # 语法: { "源字段" => "grok pattern" }
      match => { 
        "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{DATA:environment}\.%{LOGLEVEL:level}: %{GREEDYDATA:log_message}"
      }
      
      # tag_on_failure: 匹配失败时添加的标签
      # 作用: 在 Kibana 中可以过滤出解析失败的日志进行排查
      tag_on_failure => ["_grokparsefailure_laravel"]
      
      # 其他可选配置:
      # overwrite => ["message"]              # 是否覆盖原字段
      # named_captures_only => true           # 只保留命名捕获的字段
      # keep_empty_captures => false          # 是否保留空值捕获
      # timeout_millis => 30000               # 超时时间（毫秒）
    }
    
    # -------------------------------------------------------------------------
    # Date 过滤器 - 解析时间戳
    # -------------------------------------------------------------------------
    # 作用: 将日志中的时间字符串转换为 @timestamp 字段
    # 重要性: Elasticsearch 使用 @timestamp 进行时间序列索引和查询
    if [timestamp] {
      date {
        # match: 定义时间格式匹配规则
        # 支持多种格式，按顺序尝试匹配
        # "yyyy-MM-dd HH:mm:ss": Laravel 默认格式
        # "ISO8601": ISO 标准格式（备用）
        match => ["timestamp", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
        
        # target: 解析后的时间存入哪个字段
        # @timestamp 是 ELK Stack 的标准时间字段
        target => "@timestamp"
        
        # timezone: 时区设置
        # "Asia/Shanghai": 东八区（北京时间）
        # "UTC": 协调世界时
        # 建议: 统一使用 UTC，在 Kibana 中根据用户时区显示
        timezone => "Asia/Shanghai"
        
        # 其他可选配置:
        # locale => "en"                        # 语言环境
        # tag_on_failure => ["_dateparsefailure"]  # 解析失败时的标签
      }
    }
    
    # -------------------------------------------------------------------------
    # JSON 上下文提取 (处理 Laravel 的上下文数据)
    # -------------------------------------------------------------------------
    # 作用: Laravel 日志通常在消息后包含 JSON 格式的上下文信息
    # 示例: "用户登录成功 {"user_id":123,"ip":"192.168.1.1"}"
    # 目标: 提取 JSON 部分并解析为独立字段
    
    # 步骤 1: 从 log_message 中分离出 JSON 部分
    if [log_message] =~ /\{.*\}/ {
      # Grok 匹配: 将消息分为文本部分和 JSON 部分
      grok {
        match => { 
          "log_message" => "%{DATA:message_text}\s*%{GREEDYDATA:json_context}" 
        }
        tag_on_failure => ["_json_extract_failure"]
      }
      
      # 步骤 2: 解析 JSON 字符串为结构化数据
      if [json_context] =~ /^\s*\{/ {
        json {
          # source: 要解析的 JSON 字符串字段
          source => "json_context"
          
          # target: 解析后的数据存入哪个对象
          # "context": 存入 context 对象，如 context.user_id
          # "": 存入根级别，如 user_id
          target => "context"
          
          # skip_on_invalid_json: 无效 JSON 时跳过而不是报错
          skip_on_invalid_json => true
          
          # 其他可选配置:
          # tag_on_failure => ["_jsonparsefailure"]  # 解析失败时的标签
        }
      }
    }
    
    # -------------------------------------------------------------------------
    # 日志级别标签 (用于快速过滤错误日志)
    # -------------------------------------------------------------------------
    # 作用: 为错误级别的日志添加 "error" 标签
    # 用途: 在 Kibana 中可以快速过滤 tags:error 查看所有错误
    # PSR-3 日志级别: DEBUG < INFO < NOTICE < WARNING < ERROR < CRITICAL < ALERT < EMERGENCY
    if [level] == "ERROR" or [level] == "CRITICAL" or [level] == "ALERT" or [level] == "EMERGENCY" {
      mutate {
        # add_tag: 添加标签到 tags 数组
        add_tag => ["error"]
        
        # 其他可选操作:
        # add_field => { "severity" => "high" }     # 添加严重级别字段
        # add_field => { "alert_required" => true } # 标记需要告警
      }
    }
    
    # -------------------------------------------------------------------------
    # 日志频道识别 (根据文件路径判断日志类型)
    # -------------------------------------------------------------------------
    # 作用: 根据日志文件名识别日志频道，用于索引分类
    # 注意: [log][file][path] 是 Filebeat 自动添加的字段
    
    # API 频道: api.log 或 api-*.log
    if [log][file][path] =~ /api\.log/ {
      mutate {
        add_field => { "log_channel" => "api" }
      }
    } 
    # 支付频道: payment.log
    else if [log][file][path] =~ /payment\.log/ {
      mutate {
        add_field => { "log_channel" => "payment" }
      }
    } 
    # 请求频道: request.log
    else if [log][file][path] =~ /request\.log/ {
      mutate {
        add_field => { "log_channel" => "request" }
      }
    } 
    # 默认频道: laravel.log 或其他
    else {
      mutate {
        add_field => { "log_channel" => "default" }
      }
    }
    
    # -------------------------------------------------------------------------
    # 字段清理 (删除临时字段)
    # -------------------------------------------------------------------------
    # 作用: 删除处理过程中产生的临时字段，减少存储空间
    mutate {
      # remove_field: 要删除的字段列表
      remove_field => ["timestamp", "json_context"]
      
      # 其他常见的可删除字段:
      # - "message": 原始消息（已解析为结构化字段）
      # - "host": 主机信息（可能重复）
      # - "log.file": 文件信息（可能不需要）
      
      # 其他字段操作:
      # rename => { "old_field" => "new_field" }     # 重命名字段
      # lowercase => ["field1", "field2"]            # 转小写
      # uppercase => ["field1"]                      # 转大写
      # strip => ["field1"]                          # 去除首尾空格
    }
  }
  
  # ---------------------------------------------------------------------------
  # GeoIP 解析 (可选 - IP 地理位置富化)
  # ---------------------------------------------------------------------------
  # 作用: 根据 IP 地址添加地理位置信息（国家、城市、经纬度等）
  # 应用场景: 分析用户地理分布、检测异常登录位置
  # 要求: Logstash 需要安装 GeoIP 数据库
  if [context][ip] {
    geoip {
      # source: IP 地址字段
      source => "[context][ip]"
      
      # target: 地理信息存入哪个对象
      target => "geoip"
      
      # 添加的字段包括:
      # - geoip.country_name: 国家名称
      # - geoip.city_name: 城市名称
      # - geoip.location: 经纬度坐标 {lat, lon}
      # - geoip.region_name: 省/州名称
      # - geoip.timezone: 时区
      
      # 其他可选配置:
      # database => "/etc/logstash/GeoLite2-City.mmdb"  # 自定义数据库路径
      # cache_size => 1000                               # 缓存大小
      # tag_on_failure => ["_geoip_lookup_failure"]     # 查询失败时的标签
    }
  }
  
  # ---------------------------------------------------------------------------
  # Ruby 脚本处理器 (高级 - 添加处理时间)
  # ---------------------------------------------------------------------------
  # 作用: 使用 Ruby 代码添加自定义逻辑
  # 示例: 记录日志被 Logstash 处理的时间
  ruby {
    # code: Ruby 代码
    # event: 代表当前日志事件对象
    # Time.now.utc.iso8601: 获取当前 UTC 时间的 ISO8601 格式字符串
    code => "event.set('processed_at', Time.now.utc.iso8601)"
    
    # 其他 Ruby 示例:
    # 计算处理延迟
    # code => "
    #   log_time = event.get('@timestamp')
    #   process_time = Time.now
    #   event.set('processing_delay_seconds', process_time - log_time.to_time)
    # "
    
    # 字符串操作
    # code => "event.set('uppercase_level', event.get('level').upcase)"
    
    # 条件逻辑
    # code => "
    #   if event.get('level') == 'ERROR'
    #     event.set('priority', 'high')
    #   else
    #     event.set('priority', 'normal')
    #   end
    # "
  }
  
  # ---------------------------------------------------------------------------
  # 其他常用过滤器示例 (根据需要取消注释)
  # ---------------------------------------------------------------------------
  
  # User-Agent 解析 (解析浏览器、操作系统信息)
  # if [context][user_agent] {
  #   useragent {
  #     source => "[context][user_agent]"
  #     target => "user_agent"
  #   }
  # }
  
  # URL 解析 (解析 URL 的各个组成部分)
  # if [context][url] {
  #   url {
  #     source => "[context][url]"
  #     target => "url_parts"
  #   }
  # }
  
  # 字符串替换 (替换敏感信息)
  # mutate {
  #   gsub => [
  #     "log_message", "password=\S+", "password=***",
  #     "log_message", "api_key=\S+", "api_key=***"
  #   ]
  # }
  
  # DNS 反向查找 (根据 IP 查找域名)
  # if [context][ip] {
  #   dns {
  #     reverse => ["[context][ip]"]
  #     action => "replace"
  #   }
  # }
  
  # Fingerprint (生成唯一 ID，用于去重)
  # fingerprint {
  #   source => ["message", "host", "@timestamp"]
  #   target => "[@metadata][fingerprint]"
  #   method => "SHA256"
  # }
}

# =============================================================================
# OUTPUT - 数据输出配置
# =============================================================================
# 作用: 定义处理后的日志发送到哪里
# 支持: Elasticsearch, File, Kafka, Redis, S3, HTTP 等多种输出
# 注意: 可以同时配置多个输出目标

output {
  # ---------------------------------------------------------------------------
  # Elasticsearch 输出 (主要输出目标)
  # ---------------------------------------------------------------------------
  # 作用: 将日志存储到 Elasticsearch 进行索引和查询
  elasticsearch {
    # -------------------------------------------------------------------------
    # 连接配置
    # -------------------------------------------------------------------------
    # hosts: Elasticsearch 集群地址列表
    # 格式: ["host1:port", "host2:port", ...]
    # 注意: 
    #   - 使用容器名或服务名（Docker 内部 DNS）
    #   - 支持多个节点（负载均衡和高可用）
    #   - 不需要 http:// 前缀（Logstash 会自动添加）
    hosts => ["elasticsearch:9200"]
    
    # -------------------------------------------------------------------------
    # 索引配置 (重要)
    # -------------------------------------------------------------------------
    # index: 索引名称模板
    # 作用: 决定日志存储到哪个索引
    # 
    # 当前策略: 按日志频道和日期分索引
    # 格式: laravel-logs-{channel}-{date}
    # 示例:
    #   - laravel-logs-api-2026.01.20      (API 日志)
    #   - laravel-logs-payment-2026.01.20  (支付日志)
    #   - laravel-logs-default-2026.01.20  (默认日志)
    #
    # 优点:
    #   - 按业务分类，便于管理和查询
    #   - 按日期分片，便于清理旧数据
    #   - 可以为不同频道设置不同的索引策略
    #
    # 占位符说明:
    #   - %{[field]}: 引用事件中的字段值
    #   - %{+FORMAT}: 格式化 @timestamp 字段
    #   - %{+YYYY.MM.dd}: 按日分索引（推荐）
    #   - %{+YYYY.MM}: 按月分索引
    #   - %{+YYYY-ww}: 按周分索引
    index => "laravel-logs-%{[log_channel]}-%{+YYYY.MM.dd}"
    
    # 其他索引策略示例:
    # 按环境分索引: "laravel-%{[environment]}-logs-%{+YYYY.MM.dd}"
    # 按日志级别: "laravel-%{[level]}-logs-%{+YYYY.MM.dd}"
    # 单一索引: "laravel-logs" (不推荐，数据量大时性能差)
    
    # -------------------------------------------------------------------------
    # 认证配置
    # -------------------------------------------------------------------------
    # user: Elasticsearch 用户名
    # password: Elasticsearch 密码
    # 
    # 说明:
    #   - logstash_internal: docker-elk 项目预设的 Logstash 专用用户
    #   - 权限: 包含 create_index, write, manage_ilm 等必要权限
    #   - 环境变量: ${VAR_NAME} 从环境变量读取（推荐）
    #   - 安全: 避免在配置文件中硬编码密码
    #
    # 查看用户权限:
    #   GET /_security/user/logstash_internal
    user => "logstash_internal"
    password => "${LOGSTASH_INTERNAL_PASSWORD}"
    
    # -------------------------------------------------------------------------
    # 其他 Elasticsearch 配置 (可选)
    # -------------------------------------------------------------------------
    
    # 文档 ID 配置（用于去重）
    # document_id => "%{[@metadata][fingerprint]}"
    
    # 索引生命周期管理 (ILM)
    # ilm_enabled => true                          # 启用 ILM
    # ilm_rollover_alias => "laravel-logs"         # 滚动别名
    # ilm_pattern => "{now/d}-000001"              # 索引模式
    # ilm_policy => "laravel-logs-policy"          # ILM 策略名称
    
    # 管道配置（在 ES 中进一步处理）
    # pipeline => "laravel-logs-pipeline"
    
    # 批量写入配置（性能调优）
    # bulk_max_size => 500                         # 批量大小（事件数）
    # flush_size => 500                            # 刷新阈值
    # idle_flush_time => 1                         # 空闲刷新时间（秒）
    
    # 超时配置
    # timeout => 60                                # 请求超时（秒）
    # request_timeout => 60                        # HTTP 请求超时（秒）
    
    # 重试配置
    # retry_max_interval => 64                     # 最大重试间隔（秒）
    # retry_initial_interval => 2                  # 初始重试间隔（秒）
    
    # SSL/TLS 配置
    # ssl => true                                  # 启用 SSL
    # cacert => "/etc/logstash/certs/ca.crt"       # CA 证书
    # ssl_certificate_verification => true          # 验证证书
    
    # 健康检查
    # healthcheck_path => "/_cluster/health"       # 健康检查路径
    
    # 模板管理
    # manage_template => true                      # 自动管理索引模板
    # template_name => "laravel-logs"              # 模板名称
    # template_overwrite => true                   # 覆盖已存在的模板
  }
  
  # ---------------------------------------------------------------------------
  # 标准输出 (开发和调试用)
  # ---------------------------------------------------------------------------
  # 作用: 将日志输出到 Logstash 的控制台
  # 用途:
  #   - 开发环境: 实时查看处理后的日志结构
  #   - 调试: 验证过滤器是否正确工作
  #   - 故障排查: 检查数据是否正确解析
  # 
  # 建议: 生产环境注释掉此输出，避免产生大量控制台日志
  # 查看方法: docker logs docker-elk_logstash_1 -f
  stdout {
    # codec: 输出格式
    # rubydebug: 美化的调试格式，便于阅读
    # 其他选项:
    #   - json: JSON 格式
    #   - json_lines: 每行一个 JSON
    #   - line { format => "..." }: 自定义格式
    codec => rubydebug
    
    # 其他可选配置:
    # codec => json_lines                          # JSON Lines 格式（生产推荐）
    # codec => line {
    #   format => "[%{level}] %{log_message}"      # 自定义简洁格式
    # }
  }
  
  # ---------------------------------------------------------------------------
  # 其他输出示例 (根据需要取消注释)
  # ---------------------------------------------------------------------------
  
  # 文件输出 (用于归档或备份)
  # file {
  #   path => "/var/log/logstash/laravel-%{+YYYY-MM-dd}.log"
  #   codec => json_lines                          # 每行一个 JSON
  #   flush_interval => 5                          # 刷新间隔（秒）
  # }
  
  # Kafka 输出 (用于流式处理或多级架构)
  # kafka {
  #   bootstrap_servers => "kafka1:9092,kafka2:9092"
  #   topic_id => "laravel-logs"
  #   codec => json
  #   compression_type => "gzip"                   # 压缩类型
  #   acks => "1"                                  # 确认策略
  # }
  
  # Redis 输出 (用于队列或缓冲)
  # redis {
  #   host => "redis"
  #   port => 6379
  #   data_type => "list"
  #   key => "laravel:logs"
  #   codec => json
  # }
  
  # S3 输出 (用于长期存储或归档)
  # s3 {
  #   region => "us-east-1"
  #   bucket => "my-logs-bucket"
  #   prefix => "laravel-logs/%{+YYYY}/%{+MM}/%{+dd}/"
  #   size_file => 104857600                       # 100MB 一个文件
  #   time_file => 15                              # 15 分钟一个文件
  #   codec => json_lines
  # }
  
  # HTTP 输出 (用于发送到 Webhook 或其他 API)
  # http {
  #   url => "https://example.com/webhook"
  #   http_method => "post"
  #   format => "json"
  #   headers => {
  #     "Authorization" => "Bearer ${API_TOKEN}"
  #   }
  # }
  
  # 条件输出 (根据日志级别或内容选择不同的输出)
  # if [level] == "ERROR" {
  #   # 错误日志额外发送到告警系统
  #   http {
  #     url => "https://alerting.example.com/webhook"
  #     format => "json"
  #   }
  # }
  
  # if [log_channel] == "payment" {
  #   # 支付日志额外备份到文件
  #   file {
  #     path => "/var/log/logstash/payment-backup-%{+YYYY-MM-dd}.log"
  #     codec => json_lines
  #   }
  # }
}

# =============================================================================
# Pipeline 测试和验证
# =============================================================================
# 1. 验证配置文件语法:
#    docker exec docker-elk_logstash_1 logstash --config.test_and_exit \
#      --path.config /usr/share/logstash/pipeline/
#
# 2. 查看 Pipeline 统计信息:
#    curl -XGET 'http://localhost:9600/_node/stats/pipelines?pretty'
#
# 3. 热重载配置（不重启容器）:
#    docker exec docker-elk_logstash_1 kill -SIGHUP 1
#
# 4. 查看 Logstash 日志:
#    docker logs docker-elk_logstash_1 -f
#
# 5. 测试输入（使用 TCP 输入）:
#    echo '{"message":"test log","app":"laravel"}' | nc localhost 50000
#
# 6. 查看处理速度:
#    curl -XGET 'http://localhost:9600/_node/stats/pipelines?pretty' | \
#      grep -A 5 "throughput"
#
# 7. 检查 Elasticsearch 索引:
#    curl -u elastic:Es123456 "http://localhost:9200/_cat/indices/laravel-logs-*?v"
#
# 8. 查看索引文档数量:
#    curl -u elastic:Es123456 \
#      "http://localhost:9200/laravel-logs-*/_count?pretty"
# =============================================================================
